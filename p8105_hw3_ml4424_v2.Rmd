---
title: "p8105_hw3_ml4424"
author: "Maggie Li (ml4424)"
date: "10/9/2020"
output: github_document
---

## Problem 1

```{r load data}
library(p8105.datasets)
library(tidyverse)
library(utils)
library(ggplot2)
library(patchwork)
library(ggridges)
library(lubridate)


data("instacart")

## more descriptives for illustrative examples
length(unique(instacart$product_id))
length(unique(instacart$user_id))
length(unique(instacart$department))
```

**Description**: This dataset contains 1,384,617 observations that represent a unique product from an Instacart order in 2017. These data appear nested: these are products within orders within customers. There are 15 total variables; key variables include identifying the order that product came from, identifying the product, the order in which the item was added to the cart, if the product was a reorder, identifying the customer, order sequence number of the specific user, the day of week the order was placed, the hour of week the order was placed, the days that have passed since the last order by the customer, the name of the product, identifying the aisle and department of the product and the names of the aisle and department.

There are 131,209 users ordering 39,123 unique products from 21 unique departments such as dairy, canned goods, produce, bulk, bakery, household, etc.

```{r part 1}
## How many aisles are there?
length(unique(instacart$aisle_id))

## which aisles are the most items ordered from?
instacart %>% 
  count(aisle, name = "n_obs") %>% 
  arrange(desc(n_obs)) %>% 
  slice_head()
```

**Comment**: There are 134 unique aisles. The fresh vegetable aisle has the most items ordered from it.

```{r part 2}
## Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

## aisles with more than 10,000 items. there are 39 unique aisles
select_aisles <- instacart %>% 
  count(aisle, name = "n_obs") %>% 
  filter(n_obs > 10000)

##check names of aisles
unique(select_aisles$aisle)

## plot select aisles
aisles_plot <- ggplot(select_aisles, 
                      aes(x=aisle, y = n_obs, fill = aisle)) +
  geom_bar(stat="identity") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  labs(
    title = "Number of Items in Aisles with over 10,000 items",
    y = "Number of Items",
    caption = "Data from Instacart")

aisles_plot

```

**Comment**: The two most popular items by an order of magnitude than the rest are fresh fruits and fresh vegetables. Packaged vegetable fruits, packaged cheese, yogurt, water seltzer sparkling water follow after in number of products sold. The mean number of items sold out of all these categories is 27,281 items and the median number is 16,201 items. Most categories had fewer than 25,000 total items sold within them.

```{r part 3}
## Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

## baking ingredients
baking_top3 <- instacart %>% 
  filter(aisle == "baking ingredients") %>% 
  count(product_name, name = "n_obs") %>% 
  arrange(desc(n_obs)) %>% 
  head(3) %>% #select top 3
  mutate(aisle = "baking ingredients") #ID col once joined to other tbls
baking_top3

## dog food care
dogfood_top3 <- instacart %>% 
  filter(aisle == "dog food care") %>% 
  count(product_name, name = "n_obs") %>% 
  arrange(desc(n_obs)) %>% 
  head(3) %>% #select top 3
  mutate(aisle = "dog food care") #ID col once joined to other tbls
dogfood_top3

## packaged vegetables fruits
pckgd_top3 <- instacart %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  count(product_name, name = "n_obs") %>% 
  arrange(desc(n_obs)) %>% 
  head(3) %>% #select top 3
  mutate(aisle = "packaged vegetables fruits") #ID col once joined to other tbls

pckgd_top3

##join these to make a single table
top3 <- rbind(baking_top3, dogfood_top3, pckgd_top3) %>%
  rename(num_times_ordered = n_obs)
top3

```
**Comment**: Light brown sugar, pure baking soda, and cane sugar were all the top items sold in baking ingredients, with 300-500 total orders containing each. Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken & Brown Rice Recipe, and small dog biscuits were the top items sold in dog food care, with 26-30 total orders containing each. Organic baby spinach, organic raspberries, and organic blueberries were the top packaged vegetable fruit items sold, with 4900-9800 total orders containing each.

```{r part 4}
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

## Note: each column is apple and coffee, each row is day of the week, each cell is mean hour of day they are ordered on each day of week 

pl_apples <- instacart %>% 
  filter(str_detect(product_name, 'Pink Lady Apples')) %>% 
  group_by(order_dow) %>% 
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  dplyr::rename(pink_lady_apples = mean_hr)
pl_apples

coffee_ic <- instacart %>% 
  filter(str_detect(product_name, 'Coffee Ice Cream')) %>% 
  group_by(order_dow) %>% 
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  dplyr::rename(coffee_ice_cream = mean_hr)
coffee_ic

mean_hrs_all <- inner_join(pl_apples, coffee_ic) %>% 
  mutate(order_dow = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
    "Friday", "Saturday"))
mean_hrs_all
```
**Comment**: On Sunday, pink lady apples and coffee ice cream were both ordered on average a bit after at 1pm; on Monday, on average pink lady apples were ordered a bit after 11am and coffee ice cream close to 2pm; on Tuesday, on average pink lady apples were ordered around noon and coffee ice cream a bit after 3pm; on Wednesday, on average pink lady apples were ordered around 2pm and coffee ice cream a bit after 3pm; on Thursday, on average pink lady apples were ordered at noon and coffee ice cream a bit before 3pm; on Friday, on average pink lady apples were ordered close to 1pm and coffee ice cream a bit after noon; on Saturday, on average pink lady apples were ordered around noon and coffee ice cream ordered at 2pm.

Sales were overall higher for coffee ice cream than pink lady apples across all day sof the week. These patterns seem to fit with the trend that ice cream tends to be more frequently ordered later in the day than healthier snacks.

## Problem 2
```{r part 1 load tidy wrangle data}
## read and tidy
accel <- read_csv("prob2_data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  pivot_longer(cols = starts_with("activity"),
               names_to = "minute_of_day",
               values_to = "activity_ct") # onvert wide to long, each row is a minute of day


## add weekday vs weekend variable
accel <- accel %>% 
  mutate(weekday = ifelse(day %in% c('Monday','Tuesday',
                                     'Wednesday', 'Thursday',
                                     'Friday'),
                          "yes", "no")) 

## check that there are 1440 minutes in a 24-hour period
length(unique(accel$minute_of_day)) 
```

**Description**: The dataset contains a variable for week number (1-5), day_id to indicate study day (1-35) and day to indicate day of week, minute of the day (1-1440), the activity counts for each minute, and whether the activity count was recorded on a weekday or not. There are 50,400 data entries (7 days/week * 5 weeks * 1440 minutes/day).

```{r part 2 aggregate and table}
## aggregate across minutes to create total activity count per day in table
daily_accel <- accel %>% 
  group_by(day_id) %>% 
  summarize(daily_ct = sum(activity_ct))
daily_accel

## plot it to spot any trends
ggplot(daily_accel, aes(x = day_id, y = daily_ct)) +
  geom_point()
```
**Trends**: There does not appear to be any trends present when we look at aggregated daily activity count. The values fluctuate up and down day-to-day across the study period. Most daily total activity counts are around 400,000, but there are days that fluctuate substantially as well, due to missing data, changes in activity during that day, or potentially measurement error.

```{r part 3-- plot 24 hr activity time by dow}
## order days of the week, convert minute to numeric for plot
accel <- accel %>% 
  mutate(day = factor(day, levels = c('Monday','Tuesday',
                                     'Wednesday', 'Thursday',
                                     'Friday', 'Saturday', 'Sunday'))) %>% 
  mutate(minute = as.numeric(str_extract(minute_of_day, "[^_]+$")))

## convert add hours column
accel_3 <- accel %>% 
  mutate(hour = minute %/% 60,
         hour = as.integer(hour),
         study_day = paste(week, day)) %>% 
  group_by(study_day, hour) %>% 
  summarize(mean_activity = mean(activity_ct))

## add day of week identifier
accel_3_dow <- accel_3 %>% 
  mutate(dow = str_extract(study_day, "[^ ]+$"),
         dow = factor(dow, levels = c('Monday','Tuesday',
                                     'Wednesday', 'Thursday',
                                     'Friday', 'Saturday', 'Sunday'))) 
  
length(unique(accel$minute)) # check we still have 1440 min in 24-h day

## plot: should have 35 lines with 7 diff colors
accel_3_dow %>% ggplot(aes(x = hour,
                  y= mean_activity,
                  color = dow,
                  group = study_day)) +
  theme_linedraw() +
  # geom_point(aes(alpha = 0.5)) + ##cleaner without all the points
  geom_line() +
    labs(title = "Accelerometer Counts across 35 days",
       x = "Hour of Day",
       y = "Mean Hourly Activity Count") + # clean axis/titles
  scale_color_hue(name = "Study Day") # clean legend title
  
```
**Conclusions**: Consistently across all days of the week and study period, the activity count values dip at the end of the day, near midnight and stay low until rising again around 5am across all days of the week. The average activity count during the 24-h period across all study days appears to be around or a little below 500.

There appears to be differences in activity count throughout the day, and also by day of the week. Values tend to be higher on weekends in the late morning hours (10-11am) and afternoon (4-5pm). The weekday values are more similar to one another, with slight peaks in the early morning hours (6-7am) and a higher peak in the late evening around 8pm.

There also appears to be differences between given days of the week across the five weeks; for example, the Sunday hourly activity counts vary week-to-week, with some weeks having higher peaks (2 Sundays around 8am and 11am) or more normal (the rest of the Sundays during those times) mean hourly activity counts.

## Problem 3
```{r read data, cache = TRUE}
ny_noaa <- data("ny_noaa")

## check how much data are missing
ny_noaa %>% summarize_all(~sum(is.na(.)))
```
**Description**: The original data contain the ID of the weather station, the date the data were collected, and the following five metrics: precipitation level (1/10 mm), snowfall level (mm), snow depth (mm), maximum temperature (1/10 degrees C), minimum temperature (1/10 degrees C). There are a total of 2,595,176 unique daily observations. A bit more than half the observations (1,222,433) contain at least one missing metric out of the five.

```{r part 1 clean data and commonly observed snowfall}

## separate month date year; mutate precip to mm to be same as snow; tmax and tmin to degrees C instead of 10ths of degree
cleaned_ny_noaa <- ny_noaa %>% 
  separate(date, c("year", "month", "date")) %>% 
  mutate(prcp = prcp/10,
         tmax = as.integer(tmax)/10,
         tmin = as.integer(tmin)/10)
cleaned_ny_noaa

## drop weird values (negative snowfall)
cleaned_ny_noaa <- cleaned_ny_noaa %>% 
  filter(snow >= 0) 

## most commonly observed snowfall values
summary(cleaned_ny_noaa) # note that 1st quartile, median, and 3rd quartile snowfall are all 0mm
nrow(cleaned_ny_noaa %>% filter(snow == 0))
```

**Comments**: The most commonly observed value for snowfall is 0mm. 2,008,508 observations out of 2,213,954 total observations observed no snowfall. This is to be expected, because in New York State, it typically only snows a few months a year and there isn't particularly heavy snowfall across the state's region.

```{r part 2 two panel plot of max jan july temps}
## filter jan and july months
jan_jul_temps <- cleaned_ny_noaa %>% 
  filter(month == "01" | month == "07") %>% 
  select(id, year, month, date, tmax) %>% 
  drop_na()

## aggregate to get df of year, month (01 or 07), mean tmax; make year a double for plotting
jan_jul_temps
summary(jan_jul_temps)

jan_jul_mean_temps <- jan_jul_temps %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  mutate(year = as.numeric(year))

jan_jul_mean_temps

## plot
jan_jul_mean_temps %>% ggplot(aes(x=year, y = mean_tmax, color = month)) +
  geom_point(alpha = 0.5) + 
  geom_smooth() + 
  labs(title = "January and July Maximum Temperatures",
    y = "Year",
    x = "Maximum Temperature (C)",
    caption = "Data from NOAA") +
  facet_grid(~month) 
  

```
**Comments**: The mean maximum temperature for January is around 0 degrees C between 1981 and 2010. The mean maximum temperature for July is around 25 degrees C between 1981 and 2010. Across the monitor stations with available data, January maximum temperatures tend to fluctuate more than July temperatures, which are more stable on average across monitors stations year-to-year. In January, there appears to be more outliers both in the positive and negative direction away from the 0 degrees C line, while outliers in July tend toward the negative direction.

```{r part 3.1 showing tmax and tmin all months}

## plot annual averages of tmin and tmax to avoid plotting messy seasonal variations
annual_temps <- cleaned_ny_noaa %>%
  select(id, year, month, tmax, tmin) %>%
  drop_na() %>%
  group_by(year) %>%
  summarize(mean_tmax = mean(tmax),
            mean_tmin = mean(tmin)) %>%
  mutate(year = as.numeric(year))

## pivot into longer format (same as in lecture notes)
annual_temps_pivot <- annual_temps %>% 
  pivot_longer(mean_tmax:mean_tmin, 
               names_to = "observation", 
               values_to = "temp") 

annual_temp_plot <- ggplot() +
  geom_line(data = annual_temps_pivot,
            aes(x = year, y = temp, color = observation)) +
  labs(
    title = "Annual Mean Max and Min Temps",
    y = "Temperature (C)",
    x = "Year",
    caption = "Data from NOAA") + 
  scale_color_hue(name = "Type")
annual_temp_plot

```

```{r part 3.2 distribution of snowfall greater than 0 less than 100 annually}

## filter accordingly
cleaned_snow <- cleaned_ny_noaa %>% 
  drop_na(snow) %>% 
  filter(snow > 0,
         snow < 100) %>% 
  group_by(year, month, id) %>% 
  summarize(agg_snow = mean(snow))
cleaned_snow
summary(cleaned_snow)
# ## plot using ridges; note: not ideal
# ggplot(cleaned_snow, aes(x=agg_snow, y = year)) +
#   geom_density_ridges(scale = .85)

## plot using boxplots; note: more visually interpretable
snow_plot <- ggplot(cleaned_snow, aes(x=agg_snow, y = year)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Snowfall by Year",
    y = "Year",
    x = "Snowfall (mm)",
    caption = "Data from NOAA")
  

```

```{r part 3.3 combine plots into same panel}
annual_temp_plot + snow_plot
```
**Comments**: The annual mean maximum and minimum temperatures between 1981 and 2020 fluctuate considerably over time. That being said, the time series for both max and min temperatures run pretty parallel to one another (i.e. years of unusually high mean max temperatures also had unusually high mean min temperatures; years of unusally low mean max temperatures also had unusually low mean min temperatures). This plot averages out the seasonal variability within years, so its interpretation is most applicable to study questions that involve examining variations in mean max and min temperatures year-to-year.

Some unsually high mean max/min temperature years include 1990, 1991, 1997, 2001, 2006. Some unusally low mean max/min temperature years include 1989, 1994, 2000, 2003.

The distribution of snowfall greater than 0 mm and less than 100mm between 1981 and 2010 remains pretty even across the entire period. The median snowfall is around 30 mm and stays consistent year to year, give or take a few mm. The upper and lower quartiles are also quite consistent, with the 25th percentile around 22 mm and 75th percentile around 40 mm.

Values greater than 75 mm were considered outliers for almost all study years, except 1999 and 1994
